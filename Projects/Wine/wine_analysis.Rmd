---
title: "Statistics - Wine"
author: "IA - 04"
date: "1 de janeiro de 2019"
output:
  html_document:
    fig_height: 6
    fig_width: 10
    number_sections: yes
    toc: yes
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Parte I
```{r echo=FALSE, warning=FALSE, message=FALSE}
library(data.table)
library(dplyr)
library(ggplot2)
library(grid)
library(plotly)
library(tibble)
library(stringr)
library(rpart)
library(rpart.plot)
wine_dset <- read.csv2('BaseWine_Red_e_White2018.csv')
```
  
## Descrição de dados
  
Para começar nossa análise vamos identificar as variáveis para entender o que elas podem significar no contexto da composição de um vinho. 

```{r var_vinhos}
str(wine_dset)
```
  
"fixedacidity" -> *Acidez Fixa* : acidez é uma das características básica que tem uma contribuição  relevante para o sabor, frescura, equilíbrio e capacidade de conservação dos vinhos. Acidez Fixa é a diferença entre acidez total do vinho e sua acidez volátil.  
  
"volatileacidity" -> *Acidez Volátil* : é um componente do vinho que tipicamente cresce conforme o vinho envelhece e, em quando atinge um nível elevado, é responsável pelo aroma de vinagre. É o resultado da falta de cuidados durante a vinificação.  
  
"citricacid" -> *Ácido Cítrico* : nos vinhos o ácido cítrico tem pouca ou nenhuma presença. Nos vinhos tintos desaparece devido à ação de bactérias láticas (fermentação malolática). Sensorialmente é fresco, porém em alguns casos pode apresentar um leve final amargo.
  
"residualsugar" -> *Açucar Residual* : no processo de fermentação do vinho a levedura vai transformando o açúcar da uva em álcool. Por isso que, em teoria, quanto mais açúcar houver na uva, mais álcool haverá no vinho. Porem nem todo o açucar é transformado em alcool, e o açucar que resta no final do processo de fermentação é conhecido como açucar residual.  
  
"chlorides" -> *Cloretos* : os vinhos possuem em sua composição diversos produtos enológicos, sais e ácidos. Estes sais influenciam diretamente em sua qualidade.  
  
"freesulfurdioxide" -> *Dióxido de Enxofre Livre* : as atividade antioxidásica do Dioxido de Enxofre bloqueia a ação de enzimas oxidantes, principalmente no início do processo de elaboração, evitando reações de oxidação e o conseqüente escurecimento do vinho.  
  
"totalsulfurdioxide" -> *Dióxido de Enxofre Total* : dióxido de enxofre é usado como conservante nos vinhos.  
  
"density" -> *Densidade* : define a leveza do vinhos, e pode ser caracterizado pelo tipo da uva ou por técnicas usadas na vinificação que podem ser determinantes na concentração da bebida.  
  
"pH" -> *PH* : vinhos são naturalmente ácidos, com a maioria indo de 2,8 a 4,0. Os níveis de pH estão intrinsecamente ligados ao estilo e qualidade dos vinhos. O pH relativamente baixo, na faixa de 3,1 a 3,4, parece ser pré-requisito para a produção de vinhos de alta qualidade com solidez.  
  
"sulphates" -> *Sulfatos* : os sulfitos também tem um papel de conservantes nos vinhos e ajudam na extração dos compostos fenólicos do vinho, responsáveis pela concentração de cor e taninos.  
  
"alcohol" -> *Alchool* : o teor ácido é aquele responsável por conferir a textura e sensação de frescor. O teor ácido do vinho está diretamente ligado ao tipo de uva com o qual foi produzido, e o ideal é encontrar o equilíbrio perfeito entre o açúcar e a acidez ideal na hora da colheita.  
  
"quality" -> *Qualidade* : uma infidade de aspectos influenciam na qualidade dos vinhos, desde aspectos como o nivel de oxidação, até a contaminação da rolha. No dataset a qualidade máxima é uma escala inteira entre 0 (menor qualidade) e 10 (maior qualidade).  
  
"Vinho" -> *Vinho* : há inúmeros tipos de vinho (ex: tintos, brancos, roses, doces e espumantes), porém nossos dados contém apenas dois tipos: brancos e tintos.  
  
### Tamanho do dataset
```{r echo=TRUE, warning=FALSE, message=FALSE}
wine_dset %>% 
  group_by(Vinho) %>% 
  count()
```
  
A quantidade de dados para os vinhos brancos é aproximadamente 3 vezes maior que o vinho tinto.  
  
### Integridade dos dados
```{r echo=TRUE, warning=FALSE, message=FALSE}
wine_white_dset <- 
  wine_dset %>%
  filter(Vinho == 'WHITE')

wine_red_dset <- 
  wine_dset %>% 
  filter(Vinho == 'RED')

sum(is.na(wine_white_dset))
sum(is.na(wine_red_dset))
                      
```
  
Como as somas dos valores _NA_s, tanto para o vinho tinto quanto para o branco, podemos afirmar que __não há dados faltantes.__  
  
### Característica de cada tipo de vinho  
O vinho branco e vermelho apresentam características diferentes que definem se ele é bom ou ruim. Vamos dar uma olhada nos dados pra cada característica:  
  
__Vinho Branco__  
```{r echo=TRUE, warning=FALSE, message=FALSE}
summary(wine_white_dset)
```
  
  
__Vinho Vermelho__  
```{r echo=TRUE, warning=FALSE, message=FALSE}
summary(wine_red_dset)
```
  
#### Diferença da mediana das características por tipo de vinho 
A diferença alta em algumas característica sinaliza as particularidades que diferem o tipo de vinho (tinto ou branco). Será que por causa desta diferença faz sentido tratar esses dois vinhos como um só produto? Não seria interessante separá-los para uma maior precisão nas análise dos dados? 
```{r echo=TRUE, warning=FALSE, message=FALSE}
# Vinho Tinto
median_red_dset <-  sapply(select(wine_red_dset, -c(Vinho, id_vinho, quality)), median)
median_red_dset <- as.data.frame(median_red_dset)
median_red_dset <- rownames_to_column(median_red_dset) %>% 
  rename(Mediana = median_red_dset, Caracteristica = rowname) 

# Vinho Branco
median_white_dset <-  sapply(select(wine_white_dset, -c(Vinho, id_vinho, quality)),median)
median_white_dset <- as.data.frame(median_white_dset)
median_white_dset <- rownames_to_column(median_white_dset) %>% 
  rename(Mediana = median_white_dset, Caracteristica = rowname) 

# Diferenca
mediana_diferenca <- data.frame(
  Caracteristica = median_white_dset$Caracteristica,
  Diferenca.Mediana = abs(median_white_dset$Mediana - median_red_dset$Mediana)
)

arrange(mediana_diferenca, desc(mediana_diferenca$Diferenca.Mediana))
```
  
Nota: a ordem descendente dessas diferenças será utilizada nos plots para uma melhor visualização.
  
Vamos utilizar a análise gráfica pelo histograma para ajudar a responder essas questões acima:  
  
#### Histogramas de cada característica  
```{r echo=TRUE, warning=FALSE, message=FALSE}
plot_ly(wine_dset, y = ~totalsulfurdioxide,type = "box",
              color = ~Vinho, colors = c("red", "khaki")) %>% 
      layout(title = "Dióxido de Enxofre Total")

```
  
```{r echo=TRUE, warning=FALSE, message=FALSE}

plot_ly(wine_dset, y = ~freesulfurdioxide,type = "box",
              color = ~Vinho, colors = c("red", "khaki")) %>% 
      layout(title = "Dióxido de Enxofre Livre")

```
  
```{r echo=TRUE, warning=FALSE, message=FALSE}

plot_ly(wine_dset, y = ~residualsugar,type = "box",
              color = ~Vinho, colors = c("red", "khaki")) %>% 
      layout(title = "Açúcar Residual")

```
  
```{r echo=TRUE, warning=FALSE, message=FALSE}

plot_ly(wine_dset, y = ~fixedacidity,type = "box",
              color = ~Vinho, colors = c("red", "khaki")) %>% 
      layout(title = "Acidez Fixa")

```
  
Após a análise gráfica ficou ainda mais clara a diferença entre cada tipo de vinho (tinto ou branco), portanto, vamos pegar selecionar somente um tipo de vinho para uma análise coerente. Além disso, como o número de dados pros vinhos brancos é bem maior do que o para os vermelhos (aprox. 3 vezes maior), seria interessante utilizar os dados que oferecem mais amostras pra treinar e validar nosso modelo.  
  
### Remoção de outliers 
Com os histogramas foi possível identificar também a existência de outliers nos dados pois há pontos que estão acima do limite da _upper fence_ ou abaixo da _lower fence_. Esses limites (lower e upper) são calculados utilizando a seguinte relação:  
  
___Upper fence___:  

upper_fence = 3º Quartil + 1,5 * Amplitude inter-quartil

___Lower fence___:  

lower_fence = 1º Quartil - 1,5 * Amplitude inter-quartil

Para evitar que esses outliers influenciem em nossos modelos, vamos extraí-los:  
```{r echo=TRUE, warning=FALSE, message=FALSE}

wine_white_dset <- wine_white_dset %>% 
  select(-c(Vinho, id_vinho))

for (col in colnames(wine_white_dset)) {
  upper_fence <- quantile(wine_white_dset[,col], 0.75, names = FALSE) +(1.5 * IQR(wine_white_dset[,col]))
  lower_fence <- quantile(wine_white_dset[,col],0.25, names = FALSE) - (1.5 * IQR(wine_white_dset[,col]))
  
  idx <- which(wine_white_dset[,col] < lower_fence | wine_white_dset[,col] > upper_fence)
  for (x in idx) {
    wine_white_dset[x,col] <- NA
  }
}
```


# Parte II - (Vinhos Brancos)
  
## Regresão Linear

Indentificando os componentes principais do dataset

```{r echo=TRUE, warning=FALSE, message=FALSE}
vinhos_brancos <- wine_dset %>% filter(Vinho == 'WHITE')

vinhos_brancos <- vinhos_brancos[2:13]

fix(vinhos_brancos)

Padr_Vinhos <- scale(vinhos_brancos)
acpcor <- prcomp(Padr_Vinhos, scale = TRUE)

summary(acpcor)

plot(1:ncol(Padr_Vinhos), acpcor$sdev^2, type = "b", xlab = "Componente",
     ylab = "Variância", pch = 20, cex.axis = 0.8, cex.lab = 0.8)

```

Como podemos observar os componentes PC1, PC2, PC3 e PC4 se mostram os mais relevantes para a escolha do modelo.

```{r echo=TRUE, warning=FALSE, message=FALSE}

Vinhos2 <- vinhos_brancos %>%
  mutate(pc1 = acpcor$x[, 1]) %>%
  mutate(pc2 = acpcor$x[, 2]) %>%
  mutate(pc3 = acpcor$x[, 3]) %>%
  mutate(pc4 = acpcor$x[, 4]) 

matcor <- cor(Vinhos2)

#install.packages("corrplot")
library(corrplot)

corrplot::corrplot(matcor, method="circle", order="hclust")

```

Com base no gráfico gerado de componentes principais;

Podemos observar que cada componente gerado pelo componentes principais
tem uma certa especialidade: 

* A correlação de PC1 com alcohol,residualsugar,totalsulfurdioxide e density são maiores em relação as outras variáveis.  
* A correlação de PC2 com Ph, fixadacity são maiores em relação as outras variáveis.  
* A correlação de PC3 com citricacid, volatileacidity e fixadacity são maiores em relação as outras variáveis.  
* A correlação de PC4 com chlorides e sulphates são maiores em relação as outras variáveis.

Mediante nossa análise dos componentes principais desse dataset, vamos utilizar os modelos PC1 e PC3, por possuirem melhor
correlação com a variável qualidade: 


### Analisando o modelo

Variáveis dependentes: 
ModeloPC1 <- (quality)
ModeloPC3 <- (quality)

Variáveis independentes:
ModeloPC1 <- (alchool,residualsugar,totalsulfurdioxide,density)
ModeloPC3 <- (citricacid,volatileacidity,fixedacidity)

#### Analisando os coefficientes

Analisando os coefficientes

```{r echo=TRUE, warning=FALSE, message=FALSE}

attach(vinhos_brancos)

modelopc1 <- summary(lm(quality ~ alcohol+residualsugar+totalsulfurdioxide+density))
modelopc1

modelopc3 <- summary(lm(quality ~ citricacid+volatileacidity+fixedacidity))
modelopc3

```

Podemos observar no ModeloPC1;

quality <- 100.7 + 0.2366(alcohol) + 0.05542(residualsugar) + 0.0004428(totalsulfurdioxide) + 98.35(density)

R² <- 21,06%

Verificamos então que esse modelopc1 como tentativa de escolher um modelo que explique a variável quality, percebemos estatisticamente que os estimadores para o caso de beta 3 (totalsulfurdioxide) é muito próximo de 0. Assumindo um nível de significância a 5% de probabilidade podemos vê que o P valor de beta 3 (totalsulfurdioxide), é superior ao nível de significância.

Por esses motivos apresentados não a porque colocar totalsulfurdioxide no modelo.

Podemos observar no ModeloPC3;

Verificamos então que esse modelopc3 como tentativa de escolher um modelo que explique a variável quality, percebemos
estatisticamente que os estimadores para o caso de beta 1 (citricacid) é muito próximo de 0. Pelo mesmo motivo apresentado do modelopc1. Não a porque colocar citricacid no modelo.

quality <- 7.21420 - 0.28680(citricacid) - 1.76242(volatileacidity) - 0.12247(fixedacidity)

R² <- 51,89%
 
Reavaliando Modelos, depois da análise.

```{r echo=TRUE, warning=FALSE, message=FALSE}

modelopc1 <- summary(lm(quality ~ alcohol+residualsugar+density))
modelopc1

modelopc3 <- summary(lm(quality ~ volatileacidity+fixedacidity))
modelopc3

```

Podemos perceber agora que R² do modelopc1 é 21,02% e R² do modelopc3 é 51,87%. Por esse motivo escolhemos o modelopc3. Mesmo R² do modelopc3 não sendo muito adequado, foi o melhor que conseguimos, realizando diversos testes.

#### Verificar pressuposições

##### Residuos

```{r echo=TRUE, warning=FALSE, message=FALSE}

res <- rstandard(lm(quality ~ volatileacidity+fixedacidity))

modelo_fim <- lm(quality ~ volatileacidity+fixedacidity)

plot(predict(modelo_fim), res, xlab = "Preditor linear",ylab = "Residuos")
abline(h = 0, lty = 2)


qqnorm(residuals(modelo_fim), ylab="Residuos",xlab="Quantis teóricos",main="")
qqline(residuals(modelo_fim))
```

##### Teste de normalidade(Shapiro-Wilk)

```{r echo=TRUE, warning=FALSE, message=FALSE}

shapiro.test(res)

```

Verificando os residuos do modelo escolhido, nos mostra que pelo teste de normalidade utilizando a estatística do teste e obtendo o resultado de 0.97168 e o p valor de 2.2e-16. O valor de P do teste é pequeno por isso rejeita-se a hipótese de
normalidade dos residuos e por consequência, conclui-se que os erros não são normalmente distribuídos

## Árvore de regressão

Variáveis dependentes: 
ModeloPC1 <- (quality)
ModeloPC3 <- (quality)

Variáveis independentes:
ModeloPC1 <- (alcohol+residualsugar+density)
ModeloPC3 <- (volatileacidity+fixedacidity)

### Analisando Modelos

Modelo PC1

```{r echo=TRUE, warning=FALSE, message=FALSE}


modelopc1 <- rpart (quality ~ alcohol+residualsugar+density, data=vinhos_brancos, 
                     cp = 0.001,minsplit = 2,maxdepth=5)

summary(modelopc1)

rpart.plot(modelopc1, type=2, extra="auto", under=FALSE, clip.right.labs=TRUE,
           fallen.leaves=TRUE,   digits=2, varlen=-3, faclen=15,
           cex=NULL, tweak=1.7,
           compress=TRUE,box.palette="auto",
           snip=FALSE)

Val_pred_tree <- predict(modelopc1,interval = "prediction", level = 0.95) 
str(Val_pred_tree)


mse_tree <- mean((quality - Val_pred_tree)^2)
sqrt(mse_tree)



```

Podemos observar no ModeloPC1;

Porcetangem de importância das variáveis escolhidas

alcohol <- 53%
density <- 34%
residualsugar <-13% 

Número de Node criados 63 
Número de observações 4898

Maiores médias, levando em conta a taxa de cada variável

mean = 6.555556 | 
residualsugar < 1.75 | 
density < 0.990005 | 
alcohol < 13.65 

mean = 6.763948 |
residualsugar < 2.15 |
density < 0.98987 |
alcohol < 12.89667

mean = 6.875 |
alcohol < 13.55 |
residualsugar < 2.25 |
density < 0.98953


Percebemos que os vinhos com valor de residualsugar abaixo de 2.5 tem um potencial de qualidade maior, como também os vinhos com valor de density abaixo de 1 e alcohol abaixo de 14 também contribuem para um vinho de maior qualidade. Com tudo um pode equilibrar o outro na dosagem certa.


Modelo PC3

```{r echo=TRUE, warning=FALSE, message=FALSE}


modelopc3 <- rpart (quality ~ volatileacidity+fixedacidity, data=vinhos_brancos, 
                     cp = 0.001,minsplit = 2,maxdepth=5)

summary(modelopc3)

rpart.plot(modelopc3, type=2, extra="auto", under=FALSE, clip.right.labs=TRUE,
           fallen.leaves=TRUE,   digits=2, varlen=-3, faclen=15,
           cex=NULL, tweak=1.7,
           compress=TRUE,box.palette="auto",
           snip=FALSE)

Val_pred_tree2 <- predict(modelopc3,interval = "prediction", level = 0.95) 
str(Val_pred_tree)

mse_tree <- mean((quality - Val_pred_tree2)^2)
sqrt(mse_tree)

```

Podemos observar no ModeloPC3;

Porcetangem de importância das variáveis escolhidas

volatileacidity <- 59%
fixedacidity <- 41%

Número de Node criados 63 
Número de observações 4898

Maiores médias, levando em conta a taxa de cada variável

mean = 6.135546 |
volatileacidity < 0.195 |
fixedacidity    < 6.175

mean = 6.213192 |
fixedacidity < 7.45 |
volatileacidity < 0.1325

mean = 6.294643 |
volatileacidity < 0.525 |
fixedacidity < 5.35

Percebemos que os vinhos com valor de volatileacidity abaixo de 0.1300 tem um potencial de qualidade maior, como também os vinhos com valor de fixedacidity abaixo de 7.45 também contribuem para um vinho de maior qualidade. Com tudo um pode equilibrar o outro na dosagem certa.

## Resumo Regressão linear / Árvore de Regressão

Com base nas duas técnicas conseguimos enxergam alguns componentes que elevaram o padrão de qualidade do vinho, com alguns diferenciais para cada uma. A técnica de regressão linear parece ser mais segura em mostrar que os componentes citricacid, volatileaciditye e fixedacidity são os causadores de uma maior qualidade do vinho. Já a técnica de árvore de regressão mostra de uma forma mais transparente e fácil de enxergar que a qualidade do vinho é mais elevada por causa dos componentes residualsugar, density e alcohol. Ou seja o modelo pc3 se mostrou mais adequado na regressão linear, e o modelo pc1 se mostrou mais adequado na árvore de regressão.

No contexto geral o modelo PC1 mostrou os melhores resultados, usando a Árvore de Regressão.

# Parte III
## Regressão Logística
Para aplicarmor uma regressão logistica decidimos categorizar a qualidade dos vinhos em dois tipos, Vinhos Bons com uma qualidade igual ou superior a 7, ou vinhos ruins com uma pontuação de qualidade inferior a 7.
```{r echo=TRUE, warning=FALSE, message=FALSE}
summary(wine_white_dset$quality)
```
A razão de escolhermos 7 como nota divisora de vinhos bons e ruins é porquie a média da qualidade dos vinhos é de aproximandamente 6 e menos de um quarto dos vinhos brancos estão com nota 7 ou acima. Decidimos por categorizar como vinhos bons aqules que tem uma avaliação de qualidade acima da média.
Assim podemos rodar uma regressão logistica com as variaveis usadas anteriormente para analisarmos se o modelo gerado é aceitável.
```{r echo=TRUE, warning=FALSE, message=FALSE}
wine_white_dset <- wine_dset %>%
  filter(Vinho == 'WHITE')
Vinhos2Cat <- subset(wine_white_dset, select = -c(id_vinho))
attach(Vinhos2Cat)
Vinhos2Cat$quality_cat[quality >= 7] <- 'Bom'
Vinhos2Cat$quality_cat[quality < 7] <- 'Ruim'

Vinhos2Cat <- subset(Vinhos2Cat, select = -c(quality))

set.seed(2019)
prt <- 2/3

Vinhos2Cat$quality_cat <- as.factor(Vinhos2Cat$quality_cat)

treino <- sample(1:NROW(Vinhos2Cat), as.integer(prt*NROW(Vinhos2Cat)))

trainData <- Vinhos2Cat[treino,]
testData  <- Vinhos2Cat[-treino,]

modelo_log<-glm(quality_cat ~ alcohol+residualsugar+density, trainData, family=binomial(link=logit))
summary(modelo_log)
```
O p-value para todas as variáveis está ótimo, podemos usar esse modelo na nossa base de teste para calcularmos a acurácia.

```{r echo=TRUE, warning=FALSE, message=FALSE}

attach(testData)
Predito_teste<-predict(modelo_log, testData)


fx_predito <- cut(Predito_teste, breaks=c(-1,2.5,4), right=F)

MC <- table( quality_cat,  fx_predito , deparse.level = 2) #
show(MC) 
ACC = sum(diag(MC))/sum(MC) 
show(ACC) 

```
Aqui podemos ver a matriz de confusão, onde há 326 falsos positivos e 249 falsos negativos, com um total de 35% de acurácia. Esse modelo é melhor identificando os vinhos ruins, do que os bons, mas mesmo assim não consegue um resultado muito satisfatório.


## Arvore de Decisão
Vamos tentar com a arvore de decisão, usando as mesmas variáveis:
```{r echo=TRUE, warning=FALSE, message=FALSE}
modelo_tree <- rpart (quality_cat ~ alcohol+residualsugar+density, data=trainData, cp = 0.006,minsplit = 150,maxdepth=10)


rpart.plot(modelo_tree, type=4, extra=104, under=FALSE, clip.right.labs=TRUE,
           fallen.leaves=FALSE,   digits=2, varlen=-3, faclen=20,
           cex=0.4, tweak=1.7,
           compress=TRUE,
           snip=FALSE)

```
Parece promissor, ela classificou a grande maioria dos vinhos como ruins, vamos ver a acurácia dessa arvore:
```{r echo=TRUE, warning=FALSE, message=FALSE}
pred_class <- predict(modelo_tree ,testData , type = "class")

Campanha.matriz.de.confusao<-table(testData$quality_cat, pred_class)
Campanha.matriz.de.confusao

diagonal <- diag(Campanha.matriz.de.confusao)
perc.erro <- 1 - sum(diagonal)/sum(Campanha.matriz.de.confusao)
perc.erro
```
A arvore de decisão com essas variáveis obtem apenas 19% de acurácia, muito abaixo da regressão logistica.
# Parte IV

<!-- a) quais outras técnicas supervisionadas vocês indicariam como adequadas
para esta análise? -->

## Técnicas supervisionadas que poderiam ser usadas

### Gaussian Naive Bayes

Uma técnica que pode ser adequada para classificação dos vinhos é Gaussian Naive Bayes essa técnica existe desde os anos 1950. Pertence a uma família de algoritmos chamados classificadores probabilísticos ou probabilidade condicional, onde também assume independência entre os recursos,isso nos permite prever uma classe/categoria, com base em determinado conjunto de recursos, usando probabilidade.O Naive Bayes pode ser aplicado efetivamente para alguns problemas de classificação, apesar de sua simplicidade, o classificador faz a definição de categorias surpreendentemente bem e é freqüentemente usado devido ao fato de superar métodos de classificação mais sofisticados.

### Random forest

Quando usadas sozinhas, as árvores de decisão são propensas a overfitting. No entanto, random forest (Várias árvores de decisão) ajudam corrigindo o possível overfitting que poderia ocorrer. A técnica Random forest utiliza uma multiplicidade de árvores de decisão diferentes com previsões diferentes, uma random forest combina os resultados dessas árvores individuais para fornecer os resultados finais.A random forest aplica um algoritmo conjunto chamado ensacamento às árvores de decisão, que ajuda a reduzir a variação e o ajuste excessivo.

<!-- b) e, das técnicas Não Supervisionadas, quais? -->

## Técnicas não supervisionadas que poderiam ser usadas

### Generative Adversarial Networks (GANs)

O Funcionamento de uma GAN se baseia em duas redes neurais uma Geradora e outra com o Discriminadora, o papel da rede geradora é falsificar dados e da rede Discriminadora é identificar quais dados foram falsificados. Ambas estão aprendendo e melhorando. A rede gerador está constantemente aprendendo a criar falsificações melhores, e a rede  Discriminadora está constantemente melhorando em detectá-los.

### K-means

É uma técnica de análise de cluster que permite agrupar os dados em grupos chamados clusters. Como os rótulos não são fornecidos para cada dado de treinamento, os clusters são determinados pela similaridade dos dados um do outro, essa técnica pode ser uma boa opção para a classificação dos vinhos já que possuímos um grande número de dados

