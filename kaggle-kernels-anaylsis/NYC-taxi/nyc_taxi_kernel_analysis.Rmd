---
title: "Kaggle Kernel Analysis: NYC Taxi EDA"
author: "Raphael Prates"
date: '`r Sys.Date()`'
output: 
  html_document:
    number_sections: true
    toc: true
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, error=FALSE)
```
```{r, echo=FALSE,out.width='100%', fig.align='center'}
knitr::include_graphics('images/New-York-Taxi.jpg')
```
  
__Kernel name:__ "NYC Taxi EDA - Update: The fast & the curious"  
__Link:__ https://www.kaggle.com/headsortails/nyc-taxi-eda-update-the-fast-the-curious

# Introduction
In this project we are going to explore the kernel above showing why the author choose the functions used, comparing to others alternatives and we are going to emphasize the interesting points of the kernel.
  
## Libraries  
```{r libraries, echo=TRUE, message=FALSE, warning=FALSE}
library('ggplot2') # visualisation
library('scales') # visualisation
library('grid') # visualisation
library('RColorBrewer') # visualisation
library('corrplot') # visualisation
library('alluvial') # visualisation
library('dplyr') # data manipulation
library('readr') # input/output
library('data.table') # data manipulation
library('tibble') # data wrangling
library('tidyr') # data wrangling
library('stringr') # string manipulation
library('forcats') # factor manipulation
library('lubridate') # date and time
library('geosphere') # geospatial locations
library('leaflet') # maps
library('leaflet.extras') # maps
library('maps') # maps
library('xgboost') # modelling
library('caret') # modelling
library('profvis') # code execution time anaylsis
```
## Load data
  
### Original code
```{r load_data_original, echo=TRUE, message=FALSE, warning=FALSE}
library("data.table")
library("tibble")

train <- as.tibble(fread('train.csv'))
```

### __Comparison with others alternatives__  
In R, __read.csv__ is part of the regular functions and is used for load data.frame from a _csv_ file. But when we're dealing with a huge data.frame this function can take a long time to run. 
```{r l, echo=TRUE, message=FALSE, warning=FALSE}
print(paste("In this case the dataset is quite huge:",dim(train)[1], "rows and",
            dim(train)[2], "columns."))
```
So in this part the author used a function called __fread__ that performs much faster than _read.csv_ (check the time of each function using profvis!!).  
After that other function should be compared: __load__.
This function is used to load variables that have been stored in a _.RData_ file and runs very fast comparing with _read.csv_ and _fread_.  
When is a good ideia to use _load_?
When it's possible to use a background process to update the data.frame and save it in _.RData_ file.  
Let's take a look at the three possibilities:
```{r load_data_comparison, echo=TRUE, message=FALSE, warning=FALSE}
library("profvis")
library("data.table")
library("tibble")
library("readr")

profvis({
  # fread
  train <- as.tibble(fread("train.csv"))
  test <- as.tibble(fread("test.csv"))
  
  # read.csv
  train_readcsv <- read.csv("train.csv")
  
  # read_csv -> from "readr" package
  train_read_csv <- read_csv("train.csv")
  
  # loading RData
  save(train_readcsv, file = "train_data.RData")
  rm(train_readcsv)
  load(file = "train_data.RData")
})
```
  
### __Tibbles vs data frames__
All the information bellow was "greped" from https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html  
__Tibbles__  
"Tibbles are a modern take on data frames. They keep the features that have stood the test of time, and drop the features that used to be convenient but are now frustrating (i.e. converting character vectors to factors)."  
  
Major points:  

* It never changes an input's type (i.e., no more stringsAsFactors = FALSE!);
* It never adjusts the names of variables (i.e names with spaces will keep the whitespace. Data.frame   replaces whitespace for '.');
* When you print a tibble, it only shows the first ten rows and all the columns that fit on one screen;
* Tibbles are quite strict about subsetting. __[ ]__ always returns another tibble. Contrast this with a data frame: sometimes __[ ]__ returns a data frame and sometimes it just returns a vector.
  
## File structure and content  
```{r, echo=FALSE,out.width='25%', fig.align='center'}
knitr::include_graphics('images/sheldon.gif')
```
A brief overview of our data can summaries the descriptive statistics values of the dataset and detect abnormal items or outliers.

__For the summaries__
```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(train)
summary(test)
```
__Data overview__
```{r echo=TRUE, message=FALSE, warning=FALSE}
library("dplyr")
glimpse(train)
glimpse(test)
```
  
### Comparison with others alternatives
Another popular way to make a data overview is using _str_. It is very similar to _glimpse_ but _str_ shows less data. 
```{r echo=TRUE, message=FALSE, warning=FALSE}
str(train)
```
  
### First observations

* vendor_id only takes the values 1 or 2, presumably to differentiate two taxi companies  
We can easily check this doing:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
levels(as.factor(train$vendor_id))
```
* pickup_datetime and (in the training set) dropoff_datetime are combinations of date and time that we will have to re-format into a more useful shape  
* passenger_count takes a median of 1 and a maximum of 9 in both data sets  
* The pickup/dropoff_longitute/latitute describes the geographical coordinates where the meter was activate/deactivated  
* store_and_fwd_flag is a flag that indicates whether the trip data was sent immediately to the vendor ("N") or held in the memory of the taxi because there was no connection to the server ("Y"). Maybe there could be a correlation with certain geographical areas with bad reception?  
* trip_duration: our target feature in the training data is measured in seconds.  
  
## Missing values  
To avoid an inappropriate analysis of the data, the missing values should be analysed to measure their impact in the whole dataset.  
If the number of cases is __less than 5%__ of the sample, then the researcher __can drop them.__  
For more info about this subject: https://www.statisticssolutions.com/missing-values-in-data/  
Luckly there is no missing values in data (easy mode):  
```{r echo=TRUE, message=FALSE, warning=FALSE}
sum(is.na(train))
sum(is.na(test))
```
  
## __Combining train and test__  
Here the author did an interesting move: he combined _train_ and _test_ data sets into a single one in order to avoid a closely approach that matches just one part of data.  
__CAUTION:__ we can only combine the two data sets for a better overview but for the creation of a machine learning model we should keep _train_ and _test_ separate.  
```{r echo=TRUE, message=FALSE, warning=FALSE}
# Mutate creates dset, dropff_datetime and trip_duration columns for test dataset
# For train dataset only dset is created by mutate
# bind_rows combines the data sets into one
combine <- bind_rows(train %>% mutate(dset = "train"), 
                     test %>% mutate(dset = "test",
                                     dropoff_datetime = NA,
                                     trip_duration = NA))
combine <- combine %>% mutate(dset = factor(dset))
glimpse(combine)
```
  
## Reformating features
For our following analysis, we will turn the data and time from characters into date objects. We also recode vendor_id as a factor. This makes it easier to visualise relationships that involve these features.  
```{r echo=TRUE, message=FALSE, warning=FALSE}
library('lubridate')
train <- train %>%
  mutate(pickup_datetime = ymd_hms(pickup_datetime),
         dropoff_datetime = ymd_hms(dropoff_datetime),
         vendor_id = factor(vendor_id),
         passenger_count = factor(passenger_count))
```
  
## __Consistency check__  
__ASSUME NOTHING!__ It is worth checking whether the trip_durations are consistent with the intervals between the pickup_datetime and dropoff_datetime. Presumably the former were directly computed from the latter, but you never know. Below, the check variable shows "TRUE" if the two intervals are not consistent:   
```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>%
  mutate(check = abs(int_length(interval(dropoff_datetime,pickup_datetime)) + trip_duration) > 0) %>%
  select(check, pickup_datetime, dropoff_datetime, trip_duration) %>%
  group_by(check) %>%
  count()
```
And we find that everything fits perfectly.  
  
  
# __Individual feature visualizations__  
"Visualizations of feature distributions and their relations are key to understanding a data set, and they often open up new lines of inquiry. I always recommend to examine the data from as many different perspectives as possible to notice even subtle trends and correlations."  
  
## __Leaflet package for interative maps__ 
The author used a wonderful package for interative maps called [leaflet](#https://rstudio.github.io/leaflet/).
There a couple of models that you can try. We changed a little bit to try the [_leaflet-extras providers_](#http://leaflet-extras.github.io/leaflet-providers/preview/index.html).
```{r echo=TRUE, message=FALSE, warning=FALSE}
library(leaflet)
library(leaflet.extras)
set.seed(1234)
foo <- sample_n(train, 8e3)

  leaflet(data = foo) %>% addProviderTiles(providers$Esri.WorldTopoMap) %>%
  addCircleMarkers(~pickup_longitude, ~pickup_latitude, radius = 1,
                   color = "blue", fillOpacity = 0.3)
```
Using this visualization feature we notice the majority points of our trips (Manhattan and JFK airport).  
  
## __Trip Duration__
```{r echo=TRUE, message=FALSE, warning=FALSE}
library(ggplot2)

train %>%
  ggplot(aes(trip_duration)) +
  geom_histogram(fill = "red", bins = 150) +
  scale_x_log10() +
  scale_y_sqrt() +
  labs(title = "New York Taxi - EDA", x = "Trip Duration (s)", y = "Number of Events")
```
We find:  

* the majority of rides follow a rather smooth distribution that looks almost log-normal with a peak just short of 1000 seconds, i.e. about 17 minutes.
* There are several suspiciously short rides with less than 10 seconds duration.
* Additionally, there is a strange delta-shaped peak of trip_duration just before the 1e5 seconds mark and even a few way above it:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>%
  arrange(desc(trip_duration)) %>%
  select(trip_duration, pickup_datetime, dropoff_datetime, everything()) %>%
  head(10)
```
Those records would correspond to 24-hour trips and beyond, with a maximum of almost 12 days. I know that rush hour can be bad, but those values are a little unbelievable.  
  
## __Pickup and dropoff datetime__
Over the year, the distributions of pickup_datetime and dropoff_datetime look like this: 
mark and even a few way above it:  
```{r echo=FALSE, message=FALSE, warning=FALSE}
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```
```{r echo=TRUE, message=FALSE, warning=FALSE}
p1 <- train %>%
  ggplot(aes(pickup_datetime)) +
  geom_histogram(fill = "red", bins = 120) +
  labs(x = "Pickup dates")

p2 <- train %>%
  ggplot(aes(dropoff_datetime)) +
  geom_histogram(fill = "blue", bins = 120) +
  labs(x = "Dropoff dates")

layout <- matrix(c(1,2),2,1,byrow=FALSE)
multiplot(p1, p2, layout=layout)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
p1 <- 1;p2 <- 1
```
Fairly homogeneous, covering half a year between January and July 2016. There is an interesting drop around late January early February:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>%
  filter(pickup_datetime > ymd("2016-01-20") & pickup_datetime < ymd("2016-02-10")) %>%
  ggplot(aes(pickup_datetime)) +
  geom_histogram(fill = "red", bins = 120)

```
  
### __Raised questions from pickup_datetime data visualization__  

  __That's winter in NYC, so maybe snow storms or other heavy weather? Events like this should be taken into account, maybe through some handy external data set?__  
  
## _Passager count, vendor_id, store_and_fwd_flag, Day of the week and Hour of the Day_  
In the plot above we can already see some daily and weekly modulations in the number of trips. Let's investigate these variations together with the distributions of passenger_count and vendor_id by creating a multi-plot panel with different components:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
p1 <- train %>%
  group_by(passenger_count) %>%
  count() %>%
  ggplot(aes(passenger_count, n, fill = passenger_count)) +
  geom_col() +
  scale_y_sqrt() +
  theme(legend.position = "none")

p2 <- train %>%
  ggplot(aes(vendor_id, fill = vendor_id)) +
  geom_bar() +
  theme(legend.position = "none")

p3 <- train %>%
  ggplot(aes(store_and_fwd_flag)) +
  geom_bar() +
  theme(legend.position = "none") +
  scale_y_log10()

p4 <- train %>%
  mutate(wday = wday(pickup_datetime, label = TRUE)) %>%
  group_by(wday, vendor_id) %>%
  count() %>%
  ggplot(aes(wday, n, colour = vendor_id)) +
  geom_point(size = 4) +
  labs(x = "Day of the week", y = "Total number of pickups") +
  theme(legend.position = "none")

p5 <- train %>%
  mutate(hpick = hour(pickup_datetime)) %>%
  group_by(hpick, vendor_id) %>%
  count() %>%
  ggplot(aes(hpick, n, color = vendor_id)) +
  geom_point(size = 4) +
  labs(x = "Hour of the day", y = "Total number of pickups") +
  theme(legend.position = "none")

layout <- matrix(c(1,2,3,4,5,5),3,2,byrow=TRUE)
multiplot(p1, p2, p3, p4, p5, layout=layout)

```
```{r echo=FALSE, message=FALSE, warning=FALSE}
p1 <- 1; p2 <- 1; p3 <- 1; p4 <- 1; p5 <- 1
```
We find:  

* There are a few trips with zero, or seven to nine passengers but they are a rare exception:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>%
  group_by(passenger_count) %>%
  count()
```
* The vast majority of rides had only a single passenger, with two passengers being the (distant) second most popular option.  
* Towards larger passenger numbers we are seeing a smooth decline through 3 to 4, until the larger crowds (and larger cars) give us another peak at 5 to 6 passengers.  
* Vendor 2 has significantly more trips in this data set than vendor 1 (note the logarithmic y-axis). This is true for every day of the week.  
* We find an interesting pattern with Monday being the quietest day and Friday very busy. This is the same for the two different vendors, with vendor_id == 2 showing significantly higher trip numbers.  
* As one would intuitively expect, there is a strong dip during the early morning hours. There we also see not much difference between the two vendors. We find another dip around 4pm and then the numbers increase towards the evening.  
* The store_and_fwd_flag values, indicating whether the trip data was sent immediately to the vendor ("N") or held in the memory of the taxi because there was no connection to the server ("Y"), show that there was almost no storing taking place (note again the logarithmic y-axis):  
```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>%
  group_by(store_and_fwd_flag) %>%
  count()
y_count <- table(train$store_and_fwd_flag)['Y']/sum(table(train$store_and_fwd_flag))
paste0('Trip data stored in memory due to no connection represents ',round(y_count*100, digits = 2),'% of the values.')
```
  
## __Time series graphics__  
The trip volume per hour of the day depends somewhat on the month and strongly on the day of the week:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
p1 <- train %>%
  mutate(hpick = hour(pickup_datetime),
         Month = factor(month(pickup_datetime, label = TRUE))) %>%
  group_by(hpick, Month) %>%
  count() %>%
  ggplot(aes(hpick, n, color = Month)) +
  geom_line(size = 1.5) +
  labs(x = "Hour of the day", y = "count")

p2 <- train %>%
  mutate(hpick = hour(pickup_datetime),
         wday = factor(wday(pickup_datetime, label = TRUE))) %>%
  group_by(hpick, wday) %>%
  count() %>%
  ggplot(aes(hpick, n, color = wday)) +
  geom_line(size = 1.5) +
  labs(x = "Hour of the day", y = "count")

layout <- matrix(c(1,2),2,1,byrow=FALSE)
multiplot(p1, p2, layout=layout)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
p1 <- 1;p2 <- 1
```
We find:  

* January and June have fewer trips, whereas March and April are busier months. This tendency is observed for both vendor_ids.  
* The weekend (Sat and Sun, plus Fri to an extend) have higher trip numbers during the early morning ours but lower ones in the morning between 5 and 10, which can most likely be attributed to the contrast between NYC business days and weekend night life. In addition, trip numbers drop on a Sunday evening/night.  
Finally, we will look at a simple overview visualization of the pickup/dropoff latitudes and longitudes:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
p1 <- train %>%
  filter(pickup_longitude > -74.05 & pickup_longitude < -73.7) %>%
  ggplot(aes(pickup_longitude)) +
  geom_histogram(fill = "red", bins = 40)

p2 <- train %>%
  filter(dropoff_longitude > -74.05 & dropoff_longitude < -73.7) %>%
  ggplot(aes(dropoff_longitude)) +
  geom_histogram(fill = "blue", bins = 40)

p3 <- train %>%
  filter(pickup_latitude > 40.6 & pickup_latitude < 40.9) %>%
  ggplot(aes(pickup_latitude)) +
  geom_histogram(fill = "red", bins = 40)

p4 <- train %>%
  filter(dropoff_latitude > 40.6 & dropoff_latitude < 40.9) %>%
  ggplot(aes(dropoff_latitude)) +
  geom_histogram(fill = "blue", bins = 40)

layout <- matrix(c(1,2,3,4),2,2,byrow=FALSE)
multiplot(p1, p2, p3, p4, layout=layout)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
p1 <- 1; p2 <- 1; p3 <- 1; p4 <- 1
```
Here we had constrain the range of latitude and longitude values, because there are a few cases which are way outside the NYC boundaries. The resulting distributions are consistent with the focus on Manhattan that we had already seen on the map. These are the most extreme values from the pickup_latitude feature:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>%
  arrange(pickup_latitude) %>%
  select(pickup_latitude, pickup_longitude) %>%
  head(5)
```
```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>%
  arrange(desc(pickup_latitude)) %>%
  select(pickup_latitude, pickup_longitude) %>%
  head(5)
```
We need to keep the existence of these (rather astonishing) values in mind so that they don't bias our analysis.
  
## __Most common latitudes and longitudes for dropoff/pickup__  
__Pickup point__
```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>%
  select(pickup_longitude, pickup_latitude) %>%
  group_by(pickup_longitude, pickup_latitude) %>%
  count(sort = TRUE) %>%
  summary()
```
__Dropoff point__
```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>%
  select(dropoff_longitude, dropoff_latitude) %>%
  group_by(dropoff_longitude, dropoff_latitude) %>%
  count(sort = TRUE) %>%
  summary()
```
  
## Kmeans - Getting centers for dropoff/pickup coordinates  
<!-- "_K-Means is one of the most popular "clustering" algorithms. K-means stores k centroids that it uses to define clusters. A point is considered to be in a particular cluster if it is closer to that cluster's centroid than any other centroid.   -->
<!-- K-Means finds the best centroids by alternating between (1) assigning data points to clusters based on the current centroids (2) chosing centroids (points which are the center of a cluster) based on the current assignment of data points to clusters._"     -->
<!-- For more info: http://stanford.edu/~cpiech/cs221/handouts/kmeans.html   -->

<!-- ### Finding the best number of clusters for Pickup position   -->
<!-- ```{r echo=TRUE, message=FALSE, warning=FALSE} -->
<!-- library("plotly") -->

<!-- foo <- sample_n(train, 8e3) -->
<!-- quality_between_per_totss <- data.frame( -->
<!--   betweenss_per_totss = as.numeric(), -->
<!--   clusters = as.numeric(), -->
<!--   stringsAsFactors = FALSE -->
<!--   ) -->

<!-- quality_between_per_totss_vector <- c( -->
<!--   betweenss_per_totss = as.numeric(), -->
<!--   clusters = as.numeric() -->
<!-- ) -->

<!-- for (cluster in 1:30) { -->
<!--   kmeans_pickup <- kmeans(foo[,c("pickup_longitude", "pickup_latitude")], centers = cluster, -->
<!--                           algorithm = "Lloyd", iter.max = 500) -->
<!--   quality_between_per_totss_vector["betweenss_per_totss"] <- kmeans_pickup$betweenss/kmeans_pickup$totss -->
<!--   quality_between_per_totss_vector["clusters"] <- length(levels(as.factor(kmeans_pickup$cluster))) -->
<!--   quality_between_per_totss <- bind_rows(quality_between_per_totss,quality_between_per_totss_vector) -->
<!-- } -->
<!-- plot_ly(quality_between_per_totss, -->
<!--         x = quality_between_per_totss$clusters, -->
<!--         y = quality_between_per_totss$betweenss_per_totss) %>% -->
<!--         layout( -->
<!--           xaxis = list(title = "Number of clusters"), -->
<!--           yaxis = list(title = "Betweens/totss") -->
<!--           ) -->
<!-- ``` -->

<!-- ### Finding the best number of clusters for Dropoff position   -->
<!-- ```{r echo=TRUE, message=FALSE, warning=FALSE} -->
<!-- quality_between_per_totss <- data.frame( -->
<!--   betweenss_per_totss = as.numeric(), -->
<!--   clusters = as.numeric(), -->
<!--   stringsAsFactors = FALSE -->
<!--   ) -->

<!-- quality_between_per_totss_vector <- c( -->
<!--   betweenss_per_totss = as.numeric(), -->
<!--   clusters = as.numeric() -->
<!-- ) -->

<!-- for (cluster in 1:30) { -->
<!--   kmeans_dropoff <- kmeans(foo[,c("dropoff_longitude", "dropoff_latitude")], centers = cluster, -->
<!--                           algorithm = "Lloyd", iter.max = 1000) -->
<!--   quality_between_per_totss_vector["betweenss_per_totss"] <- kmeans_dropoff$betweenss/kmeans_dropoff$totss -->
<!--   quality_between_per_totss_vector["clusters"] <- length(levels(as.factor(kmeans_dropoff$cluster))) -->
<!--   quality_between_per_totss <- bind_rows(quality_between_per_totss,quality_between_per_totss_vector) -->
<!-- } -->
<!-- plot_ly(quality_between_per_totss, -->
<!--         x = quality_between_per_totss$clusters, -->
<!--         y = quality_between_per_totss$betweenss_per_totss) %>% -->
<!--         layout( -->
<!--           xaxis = list(title = "Number of clusters"), -->
<!--           yaxis = list(title = "Betweens/totss") -->
<!--           ) -->
<!-- ``` -->

<!-- So for this situation, __number of clusters = 10__ seems the best choice due to its position in the beginning of the stabilization of the curve.   -->

<!-- ### Find the centers of the K-Means Model   -->
<!-- ```{r echo=TRUE, message=FALSE, warning=FALSE} -->
<!-- icons_kpickup <- awesomeIcons( -->
<!--   icon = 'ios-close', -->
<!--   iconColor = 'black', -->
<!--   library = 'ion', -->
<!--   markerColor = "blue" -->
<!-- ) -->

<!-- icons_kdropoff <- awesomeIcons( -->
<!--   icon = 'ios-close', -->
<!--   iconColor = 'black', -->
<!--   library = 'ion', -->
<!--   markerColor = "orange" -->
<!-- ) -->

<!-- kmeans_pickup <- kmeans(foo[,c("pickup_longitude", "pickup_latitude")], centers = 10, -->
<!--                  algorithm = "Lloyd", iter.max = 1000) -->
<!-- kcenters_dataframe <- as.data.frame(kmeans_pickup$centers) -->
<!-- kmeans_dropoff <- kmeans(foo[,c("dropoff_longitude", "dropoff_latitude")], centers = 10, -->
<!--                          algorithm = "Lloyd", iter.max = 1000) -->
<!-- kcenters_dropoff <- as.data.frame(kmeans_dropoff$centers) -->

<!-- leaflet(data = foo) %>% addProviderTiles(providers$Esri.WorldTopoMap) %>% -->
<!--   addAwesomeMarkers(lng = kcenters_dataframe$pickup_longitude , lat = kcenters_dataframe$pickup_latitude, -->
<!--              icon = icons_kpickup, label = "Pickup") %>% -->
<!--   addAwesomeMarkers(lng = kcenters_dropoff$dropoff_longitude , lat = kcenters_dropoff$dropoff_latitude, -->
<!--                     icon = icons_kdropoff, label = "Dropoff") -->
<!-- ``` -->
  
  
#__Feature relations__  
Now it's time to examine how those features are related to each other and to our target _trip duration_.
  
## Pickup date/time vs _trip_duration_
  
* How does the variation in trip numbers throughout the day and the week affect the average trip duration?  
* Do quieter days and hours lead to faster trips?  
```{r echo=TRUE, message=FALSE, warning=FALSE}
p1 <- train %>%
  mutate(day_week = wday(pickup_datetime, label = TRUE)) %>%
  group_by(day_week, vendor_id) %>% 
  summarise(trip_duration_mean = median(trip_duration)/60) %>% 
  ggplot(aes(day_week, trip_duration_mean, color = vendor_id)) +
  geom_point(size = 4) +
  labs(x = "Day of the week",y = "Median trip duration [min]")

p2 <- train %>%
  mutate(hour_day = hour(pickup_datetime)) %>% 
  group_by(hour_day, vendor_id) %>% 
  summarise(trip_duration_mean = median(trip_duration)/60) %>% 
  ggplot(aes(hour_day, trip_duration_mean, color = vendor_id)) +
  geom_smooth(method = "loess", span = 1/2) +
  geom_point(size = 4) +
  labs(x = "Hour of the day",y = "Median trip duration [min]") +
  theme(legend.position = "none")

layout <- matrix(c(1,2), 2,1, byrow = FALSE)
multiplot(p1,p2, layout = layout)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
p1 <- 1; p2 <- 1
```
  
We find:  

* There is indeed a similar pattern as for the business of the day of the week. Vendor 2, the one with the more frequent trips, also has consistently higher trip durations than vendor 1. __It will be worth adding the vendor_id feature to a model to test its predictive importance.__  
* Over the course of a typical day we find a peak in the early afternoon and dips around 5-6am and 8pm. __The weekday and hour of a trip appear to be important features for predicting its duration and should be included in a successful model.__  
  
## Passenger count and Vendor vs _trip_duration_
  
__Are different numbers of passengers and/or the different vendors correlated with the duration of the trip?__  
```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>% 
  ggplot(aes(passenger_count, trip_duration, color = passenger_count)) +
  geom_boxplot() +
  scale_y_log10() +
  theme(legend.position = "none") +
  facet_wrap(~vendor_id) +
  labs(y = "Trip duration [s]", x = "Number of passengers")
```
  
We find:  

* Both vendors have short trips without any passagengers;  
* Vendor 1 has all of the trips beyond 24 hours, whereas vendor 2 has all of the (five) trips with more than six passengers and many more trips that approach the 24-hour limit.  
* Between 1 and 6 passengers the median trip durations are remarkably similar, in particular for vendor 2. There might be differences for vendor 1, but they are small (note the logarthmic y-axis):  

```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>% 
  ggplot(aes(trip_duration, fill = vendor_id)) +
  geom_density(position = "stack") +
  scale_x_log10()
```
  
Comparing the densities of the _trip_duration_ distribution for the two vendors we find the medians are very similar, whereas the means are likely skewed by vendor 2 containing most of the long-duration outliers:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>% 
  group_by(vendor_id) %>% 
  summarise(mean_duration = mean(trip_duration),
            median_duration = median(trip_duration))
```
  
## Store and Forward vs hour  
The original analysis compared "Store and Forward vs _trip_duration_", but I presume that the analysis for "Store and Forward" will better performed if we consider the location (longitude and latitude) and the dropoff time (paying time) due to some areas have a weaker signal and also when the possibility of the network being congested.
```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>%
  group_by(vendor_id, store_and_fwd_flag) %>%
  count()
```
  
As we can see above, Store and Forward appears only to vendor 1.  
Does the flags Y appear with a higher frequency at a specific time of day?  

```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>%
  mutate(hour_day = hour(dropoff_datetime)) %>%
  group_by(hour_day, store_and_fwd_flag) %>%
  count() %>% 
  ggplot(aes(hour_day, n, color = store_and_fwd_flag)) +
  geom_point(size = 4) +
  scale_y_log10() +
  labs(x = "Hour of the day",y = "No connection to the server occurrences") +
  ggtitle("Vendor_ID 1 - Store and Forward") +
  theme(plot.title = element_text(hjust = 0.5))
```
  
Flag Y follow the distribution frequency by hour of the Flag N, so we can't determine exactly with how many occurences per hour the problem aggravates.  
  
## Store and Forward vs Dropoff Position  
Let's take a look in the position of the Store_and_fwd_flag = Y to try to find a correlation:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>%
  filter(store_and_fwd_flag == "Y") %>% 
  leaflet() %>%
  addProviderTiles(providers$Esri.WorldTopoMap) %>%
  addCircleMarkers(~pickup_longitude, ~pickup_latitude, radius = 1,
                   color = "blue", fillOpacity = 0.3)
```
  
Visually the map showed no relationship between a certain area and the Store_and_flag = Y.  
  
# Feature engineering  
In this section we build new features from the existing ones, trying to find better predictors for our target variable.  
The new temporal features (date,month,wday, hour) are derived from the _pickup_datetime_. We got the JFK and LA Guardia airport coordinates from Wikipedia. The _blizzard_ feature is based on the external weather data.  
```{r echo=TRUE, message=FALSE, warning=FALSE}
jfk_coord <- tibble(lon = -73.778889, lat = 40.639722)
la_guardia_coord <- tibble(lon = -73.872611, lat = 40.77725)

pick_coord <- train %>% 
  select(pickup_longitude, pickup_latitude)

drop_coord <- train %>% 
  select(dropoff_longitude, dropoff_latitude)

train$dist <- distCosine(pick_coord, drop_coord)
train$bearing <- bearing(pick_coord, drop_coord)

train$jfk_dist_pick <- distCosine(pick_coord, jfk_coord)
train$jfk_dist_drop <- distCosine(drop_coord, jfk_coord)
train$lg_dist_pick <- distCosine(pick_coord, la_guardia_coord)
train$lg_dist_drop <- distCosine(drop_coord, la_guardia_coord)

train <- train %>% 
  mutate(speed = (dist/trip_duration)*3.6,
         date = date(pickup_datetime),
         month = month(pickup_datetime, label = TRUE),
         wday = wday(pickup_datetime, label = TRUE),
         wday = fct_relevel(wday,c("seg","ter", "qua", "qui", "sex", "sáb", "dom")),
         hour = hour(pickup_datetime),
         work = (hour %in% seq(8,18)) & (wday %in% c("seg","ter", "qua", "qui", "sex")),
         jfk_trip = (jfk_dist_pick < 2e3) | (jfk_dist_drop < 2e3),
         lg_trip = (lg_dist_pick < 2e3) | (lg_dist_drop < 2e3),
         blizzard = !((date < ymd("2016-01-22") | (date > ymd("2016-01-29"))))
  )
```
  
Let's take a look in the created fields:
```{r echo=TRUE, message=FALSE, warning=FALSE}
glimpse(train)
```
  
## Direct distance of the trip
From the coordinates of the pickup and dropoff points we can calculate the direct distance (as the crow flies) between the two points, and compare it to our _trip_durations_. Since taxis aren't crows (in most practical scenarios), these values correspond to the minimum possible travel distance.  
  
To compute these distances we are using the _distCosine_ function of the __geosphere package__ for the spherical trigonometry. This method gives us the sortest distance between two points on a spheical earth. For the purpose of this localised analysis we choose to ignore ellipsoidal distortion of the earth's shape. Here are the raw values of distance vs duration (based on a down-sized sample to speed up the kernel).
```{r echo=TRUE, message=FALSE, warning=FALSE}
set.seed(4321)

train %>% 
  sample_n(5e4) %>% 
  ggplot(aes(dist,trip_duration)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "Direct distance [m]", y = "Trip duration [s]")
```
  
We find:

* The distance generally increases with increasing _trip_duration_ (ooohh!! Really?! :]);
* Here, the 24-hours trips look even more suspicious and are even more likely to be artefacts in the data;
* In addition, there are number of trips with very short distances, down to 1 meter, but with a large range of apparent _trip_durations_.
  
Let's filter the data a little bit to remove the extreme (and the extremely suspicious) data points, and bin the data indo a 2-d histogram.  
This plot shows that in log-log space the _trip_duration_ is increasing slower than linear for large distance values:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>% 
  filter(trip_duration < 3600 & trip_duration > 120) %>% 
  filter(dist > 100 & dist < 100e3) %>% 
  ggplot(aes(dist,trip_duration)) +
  geom_bin2d(bins = c(500,500)) +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "Direct distance [m]", y = "Trip duration [s]")
```
  
## Travel Speed  
Distance over time is of course velocity, and by computing the average apparent velocity of our taxis we will have another diagnostic to remove bogus values.
Of course, __we won't be able to use _speed_ as a predictor for our model, since it requires knowing the travel time, but it can still be helpful in cleaning up our training data and findind other features with predictive power.__ This is the _speed_ distribution:
```{r echo=TRUE, message=FALSE, warning=FALSE}
train %>% 
  filter(speed > 2 & speed < 1e2) %>% 
  ggplot(aes(speed)) +
  geom_histogram(fill = "red", bins = 50) +
  labs(x = "Average speed [km/h] (direct distance)")
```
  
Well, after removing the most extreme values this looks way better than I would have expected. An average speed of around 15 km/h sounds probably reasonable for NYC. Everything above 50 km/h certainly requires magical cars (or highway travel). Also keep in mind that this refers to the direct distance and that the real velocity would have been always higher.  
In a similar way as the average duration per day and hour we can also investigate the average speed for these time bins:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
p1 <- train %>%
  group_by(vendor_id,wday) %>% 
  summarise(speed_median = median(speed)) %>% 
  ggplot(aes(wday,speed_median, color = vendor_id)) +
  geom_point(size = 4) +
  labs(x = "Day of the week", y = "Median speed [km/h]" )

p2 <- train %>%
  group_by(vendor_id,hour) %>% 
  summarise(speed_median = median(speed)) %>% 
  ggplot(aes(hour,speed_median, color = vendor_id)) +
  geom_smooth(method = "loess", span = 1/2) +
  geom_point(size = 4) +
  labs(x = "Hour of the day", y = "Median speed [km/h]" )
  theme(legend.position = "none")
  
p3 <- train %>% 
  group_by(wday,hour) %>% 
  summarise(speed_median = median(speed)) %>% 
  ggplot(aes(hour,wday, fill = speed_median)) +
  geom_tile() +
  labs(x = "Hour of the day", y = "Day of the week") +
  scale_fill_distiller(palette = "Spectral")

layout <- matrix(c(1,2,3,3), 2,2, byrow = TRUE)
multiplot(p1,p2,p3, layout = layout)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
p1 <- 1; p2 <- 1; p3 <- 1
```
  
We find:

* Our taxis appear to be travelling faster on the weekend and on a Monday than during the rest of the week;
* The early morning hours allow for a speedier trip, with everything from 8am to 6pm being similarly slow;
* There are almost no differences between the two vendors;
* The heatmap in the lower panel visualises how these trends combine to create a "low-speed-zone" in the middle of the day and week. __Based on this, we create a new feature work, which we define as 8am-6pm on seg-sáb.__

## Airport distance  
In our maps (above) and trip paths (below) we noticed that a number of trips began or ended at either of the two NYC airports: JFK and La Guardia. Since airports are usually not in the city center, it's reasonable to assume that the pickup/dropoff distance from the airport could be a useful predictor for longer _trip_durations_. Above, we defined the coordinates of the two airports and computed the corresponding distances:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
p1 <- train %>%
  ggplot(aes(jfk_dist_pick)) +
  geom_histogram(fill = "red", bins = 30) +
  scale_x_log10() +
  scale_y_sqrt() +
  geom_vline(xintercept = 2e3) +
  labs(x = "JFK pickup distance", y = "count" )

p2 <- train %>%
  ggplot(aes(lg_dist_pick)) +
  geom_histogram(fill = "red", bins = 30) +
  scale_x_log10() +
  scale_y_sqrt() +
  geom_vline(xintercept = 2e3) +
  labs(x = "La Guardia pickup distance", y = "count" )
  
p3 <- train %>%
  ggplot(aes(jfk_dist_drop)) +
  geom_histogram(fill = "blue", bins = 30) +
  scale_x_log10() +
  scale_y_sqrt() +
  geom_vline(xintercept = 2e3) +
  labs(x = "JFK dropoff distance", y = "count" )

p4 <- train %>%
  ggplot(aes(lg_dist_drop)) +
  geom_histogram(fill = "blue", bins = 30) +
  scale_x_log10() +
  scale_y_sqrt() +
  geom_vline(xintercept = 2e3) +
  labs(x = "La Guardia pickup distance", y = "count" )

layout <- matrix(c(1,2,3,4), 2,2, byrow = TRUE)
multiplot(p1,p2,p3,p4,layout = layout)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
p1 <- 1; p2 <- 1; p3 <- 1; p4 <- 1
```
  
Based on these numbers, we can define a JFK/La Guardia trip as having a pickup or dropoff distance of less than 2 km from the corresponding airport.  
  
What are the _trip_durations_ of these journeys?  
  
_trip_durations_. Above, we defined the coordinates of the two airports and computed the corresponding distances:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
p1 <- train %>%
  filter(trip_duration < 23*3600) %>% 
  ggplot(aes(jfk_trip, trip_duration, color = jfk_trip)) +
  geom_boxplot() +
  scale_y_log10() +
  labs(x = "JFK trip", y = "Trip Duration" ) +
  theme(legend.position = "none")

p2 <- train %>%
  filter(trip_duration < 23*3600) %>% 
  ggplot(aes(lg_trip, trip_duration, color = lg_trip)) +
  geom_boxplot() +
  scale_y_log10() +
  labs(x = "La Guardia trip", y = "Trip Duration" ) +
  theme(legend.position = "none")
  

layout <- matrix(c(1,2), 1,2, byrow = TRUE)
multiplot(p1,p2,layout = layout)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
p1 <- 1; p2 <- 1
```
  
We find that our hypothesis was correct and that trips to the airport, in particular the more distant JFK, have significantly longer average _trip_durations_. __These two features should definitely be part of our model.__  
  
  
# Data cleaning
Before we turn to the modelling it is time to clean up our training data. We have waited to do this until now to have a more complete overview of the problematic observations. The aim here is to remove trips that have improbable features, such as extreme trip duration.  
While there might also be a number of bogus trip durations in the test data we shouldn't be able to predict them in any case (unless there were some real correlations). By removing these training data values we will make our model more robust and more likely to generalise to unseen data, which is always our primary goal in machine learning.  
  
## Extreme trip durations  
Let's visualise the distances of the trips that took a day or longer. Unless someone took a taxi from NYC to LA, it's unlikely that those values are accurate. Here we make use of the maps package to draw an outline of Manhattan, where most of the trips begin or end. We then overlay the pickyp coordinates in red, and the dropoff coordinates in blue.  
To further add the trip connections (direct distance) we use another handy tool from the _geosphere package; GCiNTERMEDIANTE_ allow us to interpolate the path between two sets of coordinates.
  
### Longer than a day  
We start with the few trips that pretend to have taken several days to complete:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
day_plus_trips <- train %>%
  filter(trip_duration > 24*3600)

day_plus_trips %>% select(pickup_datetime, dropoff_datetime, speed)
```
  
```{r echo=TRUE, message=FALSE, warning=FALSE}
ny_map <- as.tibble(map_data("state", region = "new york:manhattan"))

tpick <- day_plus_trips %>% 
  select(lon = pickup_longitude, lat = pickup_latitude)

tdrop <- day_plus_trips %>% 
  select(lon = dropoff_longitude, lat = dropoff_latitude)

p1 <- ggplot() +
  geom_polygon(data=ny_map, aes(x=long, y=lat), fill = "grey60") +
  geom_point(data=tpick, aes(x=lon, y=lat), size=1, color = 'red', alpha = 1) +
  geom_point(data=tdrop, aes(x=lon, y=lat), size =1, color = 'blue', alpha = 1)

for (i in seq(1,nrow(tpick))) {
  inter <- as.tibble(gcIntermediate(tpick[i,], tdrop[i,], n=30, addStartEnd = TRUE))
  p1 <- p1 + geom_line(data=inter,aes(x=lon, y=lat), color = 'blue', alpha = .75)
}

p1 + ggtitle("Longer than a day trips in relation to Manhattan")
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
p1 <- 1
```
  
We find nothing out of the ordinary here. While a trip to JFK can seem like an eternity if your flight is boarding soon, it's unlikely to take this long in real time. The average taxi speeds don't look very likely either.  
  
__Decision:__ These values should be removed from the training data set for continued exploration and modelling.  
  
### Close to 24 hours  
The author don't think (so do I) it's inconceivable that someone takes a taxi for a trip that lasts almost a day (with breaks, of course). In very rare occasions this might happen; provided, of course, that the distance travelled was sufficiently long.  
Here we define day-long trips as taking between 22 and 24 hours, which covers a small peak in our raw _trip_duration_ distribution. Those are the top 5 direct distances (in m) among the day-long trips:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
day_trips <- train %>% 
  filter(trip_duration < 24*3600 & trip_duration > 22*3600)

day_trips %>% 
  arrange(desc(dist)) %>% 
  select(dist, pickup_datetime, dropoff_datetime,speed) %>% 
  head(5)
```
  
The top one is about 60 km, which is not particulary far. The average speed wouldn't suggest a generous tip, either. What do these trips look like on the map?  
```{r echo=TRUE, message=FALSE, warning=FALSE}
set.seed(2017)

day_trips <- day_trips %>% 
  sample_n(200)

tpick <- day_trips %>% 
  select(lon = pickup_longitude, lat = pickup_latitude)

tdrop <- day_trips %>% 
  select(lon = dropoff_longitude, lat = dropoff_latitude)

p1 <- ggplot() +
  geom_polygon(data=ny_map, aes(x=long, y=lat), fill = "grey60") +
  geom_point(data=tpick, aes(x=lon, y=lat), size=1, color = 'red', alpha = 1) +
  geom_point(data=tdrop, aes(x=lon, y=lat), size =1, color = 'blue', alpha = 1)

for (i in seq(1,nrow(tpick))) {
  inter <- as.tibble(gcIntermediate(tpick[i,], tdrop[i,], n=30, addStartEnd = TRUE))
  p1 <- p1 + geom_line(data=inter,aes(x=lon, y=lat), color = 'blue', alpha = .255)
}

p1 + ggtitle("Day-Long trips in relation to Manhattan")
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
p1 <- 1
```
  
Here we are plotting only 200 of the about 1800 connections to keep the map reasonably readable and the script fast. Pickup points are red and dropoff points are blue.  
  
We find:  

* A few longer distances stand out, but they are exceptions. The two major group of trips are those within Manhattan and those between Manhattan and the airports;
* There is little to suggest that these extreme _trip_durations_ were real;
* There is another insight here which is rather intuitive: trips to or from any of the airports (most prominently JFK) are unlikely to be very short. __Thus, the a close distance of either pickup or dropoff to the airport could be a valuable predictor for longer trip_diration.__. This is something that we took from here to the feature engineering.  
  
__Decision:__ We will remove _trip_durations longe than 22 hours from the exploration and possibly from the modelling.  
  
### Shorter than a few minutes  
On the other side of the _trip_duration_ distribution we have those rides that appear to only have lasted for a couple of minutes. While such short trips are entirely possible, let's check their durations and speeds to make sure that they are realistic.  
```{r echo=TRUE, message=FALSE, warning=FALSE}
min_trips <- train %>% 
  filter(trip_duration < 5*60)

min_trips %>% 
  arrange(dist) %>% 
  select(dist, pickup_datetime, dropoff_datetime,speed) %>% 
  head(5)
```
  
#### Zero-distance trips  
In doing so, we notice that there are a relatively large number of zero-distance trips:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
zero_dist <- train %>%
  filter(near(dist,0))

nrow(zero_dist)
```
  
What are their nominal top durations?
```{r echo=TRUE, message=FALSE, warning=FALSE}
zero_dist %>% 
  arrange(desc(trip_duration)) %>% 
  select(trip_duration, pickup_datetime, dropoff_datetime,vendor_id) %>% 
  head(5)
```
  
There really are a few taxis where the data wants to tell us they have not moved at all for about a day. While carrying a passenger. We choose not to believe the data in this case.  
Once we remove the extreme cases, this is what the distribution looks like:  
```{r echo=TRUE, message=FALSE, warning=FALSE}
zero_dist %>% 
  filter(trip_duration < 6000) %>% 
  ggplot(aes(trip_duration, fill = vendor_id)) +
  geom_histogram(bins = 50) +
  scale_x_log10()
```



